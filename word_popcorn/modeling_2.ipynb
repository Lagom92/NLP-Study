{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitnlpconda2e37df55ef7041bb8d56bf0ba20910ea",
   "display_name": "Python 3.7.6 64-bit ('NLP': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec을 활용한 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 각 단어에 대해 word2vec으로 벡터화\n",
    "- 전처리된 데이터를 불러온 후 각 단어들의 리스트로 나눠야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 전처리한 데이터의 경우 리뷰가 하나의 문자열로 구성되어 있음\n",
    "- word2vec를 사용하기 위해서는 입력값을 단어로 구분된 리스트로 만들어야함"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word2vec 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 모델의 하이퍼파라미터 설정\n",
    "# 학습 시 필요한 하이퍼파라미터\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- num_features\n",
    "    - 워드 벡터 특징값 수\n",
    "    - 각 단어에 대해 임베딩된 벡터의 차원을 정한다.\n",
    "\n",
    "- min_word_count\n",
    "    - 단어에 대한 최소 빈도 수\n",
    "    - 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.\n",
    "    \n",
    "- num_workers\n",
    "    - 프로세스 개수\n",
    "    - 모델 학습 시 학습을 위한 프로세스 개수를 지정한다.\n",
    "\n",
    "- context\n",
    "    - 컨텍스트 윈도우 크기\n",
    "    - word2vec을 수행하기 위한 컨텍스트 윈도우 크기를 지정한다.\n",
    "\n",
    "- downsampling\n",
    "    - 다운 샘플링 비율\n",
    "    - word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 로깅을 할 때 format을 위와 같이 지정하고 로그 수준은 INFO에 맞추면 word2vec의 학습 과정에서 로그 메세지를 양식에 맞게 INFO 수준으로 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- word2vec 모듈에 있는 Word2Vec 객체를 생성해서 실행\n",
    "- 학습하고 생성된 객체는 model 변수에 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2020-02-15 14:31:30,764 : INFO : collecting all words and their counts\n2020-02-15 14:31:30,767 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\nTraining model.....\n2020-02-15 14:31:31,748 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n2020-02-15 14:31:32,675 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n2020-02-15 14:31:33,124 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n2020-02-15 14:31:33,126 : INFO : Loading a fresh vocabulary\n2020-02-15 14:31:33,281 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n2020-02-15 14:31:33,282 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n2020-02-15 14:31:33,396 : INFO : deleting the raw counts dictionary of 74065 items\n2020-02-15 14:31:33,402 : INFO : sample=0.001 downsamples 30 most-common words\n2020-02-15 14:31:33,409 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n2020-02-15 14:31:33,479 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n2020-02-15 14:31:33,481 : INFO : resetting layer weights\n2020-02-15 14:31:39,401 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n2020-02-15 14:31:40,436 : INFO : EPOCH 1 - PROGRESS: at 8.24% examples, 200573 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:41,451 : INFO : EPOCH 1 - PROGRESS: at 15.66% examples, 193812 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:42,491 : INFO : EPOCH 1 - PROGRESS: at 23.18% examples, 190171 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:43,506 : INFO : EPOCH 1 - PROGRESS: at 29.53% examples, 181493 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:44,542 : INFO : EPOCH 1 - PROGRESS: at 35.72% examples, 175463 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:45,633 : INFO : EPOCH 1 - PROGRESS: at 42.46% examples, 171401 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:46,654 : INFO : EPOCH 1 - PROGRESS: at 48.74% examples, 168950 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:47,696 : INFO : EPOCH 1 - PROGRESS: at 55.23% examples, 167685 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:48,718 : INFO : EPOCH 1 - PROGRESS: at 62.33% examples, 167984 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:49,728 : INFO : EPOCH 1 - PROGRESS: at 69.63% examples, 169290 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:50,778 : INFO : EPOCH 1 - PROGRESS: at 77.34% examples, 170392 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:31:51,822 : INFO : EPOCH 1 - PROGRESS: at 85.94% examples, 173413 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:52,895 : INFO : EPOCH 1 - PROGRESS: at 94.46% examples, 174921 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:53,629 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-02-15 14:31:53,689 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-02-15 14:31:53,695 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-02-15 14:31:53,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-02-15 14:31:53,720 : INFO : EPOCH - 1 : training on 2988089 raw words (2494134 effective words) took 14.3s, 174268 effective words/s\n2020-02-15 14:31:54,802 : INFO : EPOCH 2 - PROGRESS: at 5.95% examples, 147645 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:55,818 : INFO : EPOCH 2 - PROGRESS: at 13.18% examples, 163515 words/s, in_qsize 6, out_qsize 1\n2020-02-15 14:31:56,869 : INFO : EPOCH 2 - PROGRESS: at 21.18% examples, 174679 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:57,870 : INFO : EPOCH 2 - PROGRESS: at 28.84% examples, 178610 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:31:58,903 : INFO : EPOCH 2 - PROGRESS: at 36.70% examples, 181330 words/s, in_qsize 8, out_qsize 1\n2020-02-15 14:31:59,926 : INFO : EPOCH 2 - PROGRESS: at 43.72% examples, 179462 words/s, in_qsize 6, out_qsize 1\n2020-02-15 14:32:00,931 : INFO : EPOCH 2 - PROGRESS: at 49.36% examples, 173948 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:01,972 : INFO : EPOCH 2 - PROGRESS: at 52.93% examples, 162925 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:03,100 : INFO : EPOCH 2 - PROGRESS: at 59.57% examples, 160986 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:04,149 : INFO : EPOCH 2 - PROGRESS: at 65.43% examples, 158343 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:05,162 : INFO : EPOCH 2 - PROGRESS: at 71.94% examples, 158813 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:06,206 : INFO : EPOCH 2 - PROGRESS: at 79.41% examples, 160165 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:07,211 : INFO : EPOCH 2 - PROGRESS: at 87.17% examples, 162964 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:08,285 : INFO : EPOCH 2 - PROGRESS: at 96.76% examples, 166834 words/s, in_qsize 7, out_qsize 1\n2020-02-15 14:32:08,495 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-02-15 14:32:08,515 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-02-15 14:32:08,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-02-15 14:32:08,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-02-15 14:32:08,608 : INFO : EPOCH - 2 : training on 2988089 raw words (2494522 effective words) took 14.8s, 168409 effective words/s\n2020-02-15 14:32:09,644 : INFO : EPOCH 3 - PROGRESS: at 6.88% examples, 170579 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:10,686 : INFO : EPOCH 3 - PROGRESS: at 14.04% examples, 172416 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:11,692 : INFO : EPOCH 3 - PROGRESS: at 20.52% examples, 169752 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:12,721 : INFO : EPOCH 3 - PROGRESS: at 27.24% examples, 167577 words/s, in_qsize 8, out_qsize 1\n2020-02-15 14:32:13,723 : INFO : EPOCH 3 - PROGRESS: at 34.78% examples, 171938 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:14,768 : INFO : EPOCH 3 - PROGRESS: at 43.72% examples, 179156 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:15,796 : INFO : EPOCH 3 - PROGRESS: at 51.37% examples, 180037 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:16,807 : INFO : EPOCH 3 - PROGRESS: at 58.21% examples, 179038 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:17,853 : INFO : EPOCH 3 - PROGRESS: at 65.43% examples, 177691 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:18,895 : INFO : EPOCH 3 - PROGRESS: at 71.62% examples, 175009 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:19,929 : INFO : EPOCH 3 - PROGRESS: at 77.34% examples, 171445 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:20,960 : INFO : EPOCH 3 - PROGRESS: at 84.33% examples, 171227 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:22,009 : INFO : EPOCH 3 - PROGRESS: at 91.35% examples, 170760 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:22,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-02-15 14:32:22,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-02-15 14:32:22,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-02-15 14:32:22,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-02-15 14:32:22,947 : INFO : EPOCH - 3 : training on 2988089 raw words (2494506 effective words) took 14.3s, 174197 effective words/s\n2020-02-15 14:32:23,990 : INFO : EPOCH 4 - PROGRESS: at 7.92% examples, 193475 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:25,040 : INFO : EPOCH 4 - PROGRESS: at 16.97% examples, 206971 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:26,079 : INFO : EPOCH 4 - PROGRESS: at 24.98% examples, 201743 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:27,154 : INFO : EPOCH 4 - PROGRESS: at 32.49% examples, 195543 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:28,159 : INFO : EPOCH 4 - PROGRESS: at 39.12% examples, 189565 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:29,185 : INFO : EPOCH 4 - PROGRESS: at 45.51% examples, 183577 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:30,220 : INFO : EPOCH 4 - PROGRESS: at 52.93% examples, 183676 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:31,233 : INFO : EPOCH 4 - PROGRESS: at 60.58% examples, 184237 words/s, in_qsize 6, out_qsize 1\n2020-02-15 14:32:32,240 : INFO : EPOCH 4 - PROGRESS: at 68.38% examples, 184861 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:33,280 : INFO : EPOCH 4 - PROGRESS: at 73.95% examples, 179865 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:34,363 : INFO : EPOCH 4 - PROGRESS: at 80.13% examples, 175857 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:35,505 : INFO : EPOCH 4 - PROGRESS: at 85.32% examples, 170393 words/s, in_qsize 8, out_qsize 1\n2020-02-15 14:32:36,545 : INFO : EPOCH 4 - PROGRESS: at 90.68% examples, 167118 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:37,559 : INFO : EPOCH 4 - PROGRESS: at 96.42% examples, 165095 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:38,032 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-02-15 14:32:38,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-02-15 14:32:38,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-02-15 14:32:38,132 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-02-15 14:32:38,141 : INFO : EPOCH - 4 : training on 2988089 raw words (2494720 effective words) took 15.2s, 164396 effective words/s\n2020-02-15 14:32:39,262 : INFO : EPOCH 5 - PROGRESS: at 5.28% examples, 126879 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:40,302 : INFO : EPOCH 5 - PROGRESS: at 12.52% examples, 151145 words/s, in_qsize 8, out_qsize 1\n2020-02-15 14:32:41,305 : INFO : EPOCH 5 - PROGRESS: at 19.54% examples, 160772 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:42,349 : INFO : EPOCH 5 - PROGRESS: at 26.91% examples, 164234 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:43,384 : INFO : EPOCH 5 - PROGRESS: at 33.49% examples, 163381 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:44,387 : INFO : EPOCH 5 - PROGRESS: at 39.47% examples, 161035 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:45,422 : INFO : EPOCH 5 - PROGRESS: at 45.16% examples, 157392 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:46,504 : INFO : EPOCH 5 - PROGRESS: at 51.37% examples, 155882 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:47,569 : INFO : EPOCH 5 - PROGRESS: at 56.51% examples, 152284 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:48,572 : INFO : EPOCH 5 - PROGRESS: at 62.33% examples, 151173 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:49,667 : INFO : EPOCH 5 - PROGRESS: at 68.38% examples, 149795 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:50,715 : INFO : EPOCH 5 - PROGRESS: at 74.61% examples, 149785 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:51,742 : INFO : EPOCH 5 - PROGRESS: at 80.13% examples, 148209 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:52,792 : INFO : EPOCH 5 - PROGRESS: at 85.94% examples, 147780 words/s, in_qsize 8, out_qsize 0\n2020-02-15 14:32:53,821 : INFO : EPOCH 5 - PROGRESS: at 92.05% examples, 147554 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:54,837 : INFO : EPOCH 5 - PROGRESS: at 97.78% examples, 146975 words/s, in_qsize 7, out_qsize 0\n2020-02-15 14:32:55,079 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-02-15 14:32:55,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-02-15 14:32:55,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-02-15 14:32:55,196 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-02-15 14:32:55,200 : INFO : EPOCH - 5 : training on 2988089 raw words (2494551 effective words) took 17.0s, 146930 effective words/s\n2020-02-15 14:32:55,203 : INFO : training on a 14940445 raw words (12472433 effective words) took 75.8s, 164543 effective words/s\n"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training model.....\")\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context,\n",
    "    sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기에 좋을 것이다.\n",
    "- 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 저장하기\n",
    "# model_name = \"300features_10minwords_10context\"\n",
    "# model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 선형 회귀 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 입력값 만드는 방법\n",
    "    - 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 리뷰에 대해 전체 단어의 평균값을 계산하는 함수\n",
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "\n",
    "    num_words = 0\n",
    "\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "\n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- words\n",
    "    - 단어의 모음인 하나의 리뷰가 들어간다.\n",
    "\n",
    "- model\n",
    "    - word2vec 모델을 넣는 곳이며, 우리가 학습한 word2vec 모델이 들어간다.\n",
    "\n",
    "- num_features\n",
    "    - word2vec으로 임베딩할때 정했던 벡터의 차원 수를 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 하나의 벡터를 만드는 과정에서 속도를 빠르게 하기 위해 np.zeros를 사용해 미리 모두 0의 값을 가지는 벡터를 만듬\n",
    "- 문장의 단어가 해당 모델 단어사전에 속하는지 보기 위해 model.wv.index2word를 set 객체로 생성해서 index2word_set 변수에 할당함\n",
    "- 반복문을 통해 리뷰를 구성하는 단어에 대해 임베딩된 벡터가 있는 단어 벡터의 합을 구함\n",
    "- 사용한 단어의 전체 개수로 나눔으로써 평균 벡터의 값을 구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- reviews\n",
    "    - 학습 데이터인 전체 리뷰 데이터를 입력하는 인자\n",
    "\n",
    "- model\n",
    "    - word2vec 모델을 입력하는 인자\n",
    "    - 위 함수와 같이 앞에서 학습시킨 모델을 넣는다.\n",
    "\n",
    "- num_features\n",
    "    - word2vec으로 임베딩할때 정했던 벡터의 차원 수를 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 속도 향상을 위해 전체 리뷰에 대한 평균 벡터를 담을 0 으로 채워진 넘파이 배열을 미리 만든다.\n",
    "- 배열은 2차원으로 만드는데 배열의 행에는 각 문장에 대한 길이를 입력하면 되고 열에는 평균 벡터의 차원 수, 즉 크기를 입력한다.\n",
    "- 각 리뷰에 대해 반복문을 돌면서 각 리뷰에 대해 특징 값을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 리뷰 데이터를 위 함수를 사용해 실제 학습에 사용될 입력값 만들기\n",
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.05141454, -0.09214196, -0.10656108, ..., -0.25861964,\n         0.11964124,  0.15288605],\n       [ 0.21623717, -0.07709181, -0.06491359, ..., -0.13432683,\n         0.15685375, -0.25512555],\n       [-0.12524039, -0.01853671,  0.10024042, ..., -0.17027494,\n         0.17265838,  0.00813573],\n       ...,\n       [ 0.30068046, -0.17251717, -0.15885326, ..., -0.17351472,\n         0.08153304,  0.3920877 ],\n       [ 0.1485902 ,  0.05575932, -0.0201204 , ..., -0.21828656,\n         0.08524875, -0.00382755],\n       [-0.03531034,  0.15706989,  0.06043947, ..., -0.25309756,\n         0.1035813 ,  0.11736225]], dtype=float32)"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 데이터셋과 검증 데이터셋 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 선언 및 학습\n",
    "- 로지스틱 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.07283596, -0.10686829, -0.05979674, ..., -0.3413663 ,\n         0.14170878,  0.18238851],\n       [ 0.30186263, -0.00209423, -0.21597567, ..., -0.50307095,\n        -0.14702682,  0.3526893 ],\n       [ 0.35746956, -0.20288137, -0.19624706, ..., -0.19885197,\n        -0.03129605,  0.05283298],\n       ...,\n       [ 0.0798316 , -0.06252076,  0.04627676, ...,  0.00895521,\n         0.11041459, -0.1396915 ],\n       [-0.00516012,  0.12322056, -0.11593889, ..., -0.22621252,\n         0.14311898,  0.00674704],\n       [ 0.40598226, -0.17368406, -0.21998726, ..., -0.241794  ,\n        -0.05144045,  0.04521733]], dtype=float32)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 1, 1, 0])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 검증 데이터셋을 이용한 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 0.8622\n"
    }
   ],
   "source": [
    "print(f\"Accuracy: {lgs.score(X_eval, y_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kaggle 데이터 재출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>naturally film main themes mortality nostalgia...</td>\n      <td>\"12311_10\"</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>movie disaster within disaster film full great...</td>\n      <td>\"8348_2\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>movie kids saw tonight child loved one point k...</td>\n      <td>\"5828_4\"</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afraid dark left impression several different ...</td>\n      <td>\"7186_2\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>accurate depiction small time mob life filmed ...</td>\n      <td>\"12128_7\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                              review          id\n0  naturally film main themes mortality nostalgia...  \"12311_10\"\n1  movie disaster within disaster film full great...    \"8348_2\"\n2  movie kids saw tonight child loved one point k...    \"5828_4\"\n3  afraid dark left impression several different ...    \"7186_2\"\n4  accurate depiction small time mob life filmed ...   \"12128_7\""
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터를 각 단어의 리스트로 만들기\n",
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "eheader',\n  'certainly',\n  'something',\n  'like',\n  'never',\n  'seen',\n  'movies',\n  'something',\n  'seen',\n  'shall',\n  'probably',\n  'never',\n  'forget',\n  'even',\n  'wish',\n  'could',\n  'wine',\n  'drinking',\n  'penis',\n  'roped',\n  'seen',\n  'shame',\n  'forever',\n  'things',\n  'strike',\n  'director',\n  'porn',\n  'magnate',\n  'fantasies',\n  'sprung',\n  'sick',\n  'imaginations',\n  'proved',\n  'record',\n  'acting',\n  'pretty',\n  'good',\n  'actually',\n  'think',\n  'mcdowell',\n  'weakest',\n  'link',\n  'mirren',\n  'always',\n  'something',\n  'behold',\n  'youthful',\n  'years',\n  'formidable',\n  'beautiful',\n  'portrayals',\n  'nerva',\n  'tiberius',\n  'done',\n  'well',\n  'respected',\n  'actors',\n  'film',\n  'could',\n  'done',\n  'much',\n  'better',\n  'story',\n  'telling',\n  'failure',\n  'way',\n  'whole',\n  'layer',\n  'shame',\n  'thing',\n  'example',\n  'could',\n  'given',\n  'opinion',\n  'caligula',\n  'went',\n  'mad',\n  'shown',\n  'mad',\n  'paranoid',\n  'illness',\n  'fever',\n  'broke',\n  'mental',\n  'restraint',\n  'possessed',\n  'previously',\n  'could',\n  'shown',\n  'weakness',\n  'claudius',\n  'miracle',\n  'survived',\n  'caligula',\n  'could',\n  'many',\n  'things',\n  'stuck',\n  'sad',\n  'nightmares',\n  'finally',\n  'sex',\n  'porn',\n  'shows',\n  'sex',\n  'acts',\n  'done',\n  'palaces',\n  'many',\n  'people',\n  'male',\n  'male',\n  'female',\n  'female',\n  'male',\n  'female',\n  'passionless',\n  'disgusting',\n  'sex',\n  'sex',\n  'point',\n  'get',\n  'seriously',\n  'doubt',\n  'could',\n  'tiberius',\n  'caligula',\n  'may',\n  'sex',\n  'addicts',\n  'orgies',\n  'may',\n  'common',\n  'enough',\n  'visions',\n  'film',\n  'seem',\n  'recall',\n  'greek',\n  'time',\n  'roman',\n  'think',\n  'kind',\n  'acceptance',\n  'homosexuality',\n  'orgies',\n  'right',\n  'open',\n  'common',\n  'strange',\n  'depraved',\n  'reign',\n  'elgabalus',\n  'killed',\n  'film',\n  'probably',\n  'never',\n  'watched',\n  'curious',\n  'let',\n  'go',\n  'something',\n  'said',\n  'earlier',\n  'watch',\n  'unfortunately',\n  'retain',\n  'sick',\n  'visions',\n  'minds',\n  'eye',\n  'years',\n  'come',\n  'think',\n  'accurate',\n  'historical',\n  'picture',\n  'particular',\n  'even',\n  'historical',\n  'whole'],\n ['spirit',\n  'st',\n  'louis',\n  'billy',\n  'wilder',\n  'film',\n  'tribute',\n  'one',\n  'best',\n  'figures',\n  'aeronautical',\n  'history',\n  'remembered',\n  'first',\n  'nonstop',\n  'solo',\n  'flight',\n  'across',\n  'atlantic',\n  'ocean',\n  'may',\n  'james',\n  'stewart',\n  'little',\n  'old',\n  'part',\n  'playing',\n  'charles',\n  'lindbergh',\n  'tribute',\n  'eloquent',\n  'enough',\n  'although',\n  'nice',\n  'liberties',\n  'may',\n  'taken',\n  'historical',\n  'fact',\n  'motion',\n  'picture',\n  'describing',\n  'detailed',\n  'odyssey',\n  'paris',\n  'flight',\n  'may',\n  'monoplane',\n  'spirit',\n  'st',\n  'louis',\n  'although',\n  'lengthy',\n  'internal',\n  'monologue',\n  'employed',\n  'journey',\n  'may',\n  'disappointing',\n  'audience',\n  'truth',\n  'helps',\n  'keep',\n  'picture',\n  'focused',\n  'tightly',\n  'essential',\n  'point',\n  'stewart',\n  'dignified',\n  'portrait',\n  'one',\n  'greatest',\n  'adventurers',\n  'air',\n  'world',\n  'ever',\n  'know',\n  'departing',\n  'highly',\n  'modified',\n  'single',\n  'engine',\n  'monoplane',\n  'long',\n  'island',\n  'new',\n  'york',\n  'paris',\n  'france',\n  'action',\n  'depicted',\n  'trip',\n  'flashbacks',\n  'break',\n  'monotony',\n  'long',\n  'flight',\n  'superb',\n  'determination',\n  'ordeal',\n  'brave',\n  'talented',\n  'pilot',\n  'decided',\n  'fly',\n  'alone',\n  'equation',\n  'simple',\n  'less',\n  'weight',\n  'one',\n  'engine',\n  'one',\n  'pilot',\n  'would',\n  'increase',\n  'fuel',\n  'efficiency',\n  'allow',\n  'longer',\n  'flying',\n  'range',\n  'much',\n  'risk',\n  'lindbergh',\n  'claim',\n  'fame',\n  'something',\n  'many',\n  'tried',\n  'failed',\n  'even',\n  'though',\n  'wilder',\n  'bravely',\n  'put',\n  'upon',\n  'screen',\n  'calm',\n  'unhurried',\n  'fashion',\n  'comes',\n  'biography',\n  'intense',\n  'restraint',\n  'power',\n  'james',\n  'stewart',\n  'performance',\n  'controlled',\n  'last',\n  'detail',\n  'gives',\n  'life',\n  'strong',\n  'heroic',\n  'stature',\n  'principal',\n  'figure',\n  'film',\n  'emerges',\n  'awareness',\n  'clever',\n  'firm',\n  'truly',\n  'humble',\n  'man',\n  'tackles',\n  'task',\n  'resolution',\n  'plans',\n  'much',\n  'makes',\n  'decisions',\n  'courageous',\n  'finality',\n  'awaits',\n  'one',\n  'thought',\n  'mind',\n  'get',\n  'paris',\n  'efforts',\n  'cut',\n  'plane',\n  'weight',\n  'item',\n  'considered',\n  'heavy',\n  'unnecessary',\n  'left',\n  'behind',\n  'record',\n  'setting',\n  'flight',\n  'proved',\n  'fight',\n  'elements',\n  'test',\n  'navigation',\n  'also',\n  'long',\n  'battle',\n  'fatigue',\n  'busy',\n  'schedule',\n  'active',\n  'mind',\n  'kept',\n  'lindbergh',\n  'previous',\n  'night',\n  'still',\n  'managed',\n  'stay',\n  'conscious',\n  'enough',\n  'keep',\n  'monoplane',\n  'crashing',\n  'landed',\n  'le',\n  'bourget',\n  'aerodrome',\n  'near',\n  'paris',\n  'hours',\n  'minutes',\n  'leaving',\n  'new',\n  'york',\n  'stewart',\n  'gives',\n  'able',\n  'portrait',\n  'brave',\n  'pilot',\n  'attains',\n  'legendary',\n  'status',\n  'emphasizing',\n  'intention',\n  'dominant',\n  'resolution',\n  'fly',\n  'nonstop',\n  'kilometers',\n  'miles',\n  'across',\n  'atlantic',\n  'photographed',\n  'cinemascope',\n  'warnercolor',\n  'backed',\n  'franz',\n  'waxman',\n  'beautiful',\n  'music',\n  'film',\n  'effectively',\n  'captures',\n  'pioneering',\n  'spirit',\n  'era',\n  'hero',\n  'ultimate',\n  'achievement',\n  'since',\n  'takes',\n  'day',\n  'roosevelt',\n  'wet',\n  'field',\n  'clears',\n  'telephone',\n  'wires',\n  'end',\n  'runway'],\n ['john',\n  'wayne',\n  'without',\n  'doubt',\n  'one',\n  'popular',\n  'loved',\n  'actors',\n  'time',\n  'career',\n  'stretched',\n  'forty',\n  'years',\n  'within',\n  'time',\n  'starred',\n  'films',\n  'angel',\n  'badman',\n  'green',\n  'berets',\n  'sands',\n  'iwo',\n  'jima',\n  'rio',\n  'bravo',\n  'north',\n  'alaska',\n  'undefeated',\n  'film',\n  'listed',\n  'hailed',\n  'best',\n  'unlike',\n  'effort',\n  'randy',\n  'rides',\n  'alone',\n  'pretty',\n  'much',\n  'forgotten',\n  'time',\n  'gone',\n  'unsurprising',\n  'nothing',\n  'memorable',\n  'apart',\n  'short',\n  'running',\n  'time',\n  'minutes',\n  'young',\n  'john',\n  'wayne',\n  'plays',\n  'randy',\n  'bowers',\n  'reasons',\n  'never',\n  'really',\n  'explained',\n  'arrives',\n  'saloon',\n  'middle',\n  'nowhere',\n  'finds',\n  'everyone',\n  'inside',\n  'killed',\n  'looking',\n  'around',\n  'posse',\n  'arrives',\n  'finds',\n  'randy',\n  'arrest',\n  'accusing',\n  'gang',\n  'member',\n  'demand',\n  'know',\n  'rest',\n  'gang',\n  'put',\n  'jail',\n  'accused',\n  'murders',\n  'sally',\n  'rogers',\n  'whose',\n  'uncle',\n  'owned',\n  'saloon',\n  'murdered',\n  'arrives',\n  'jail',\n  'see',\n  'randy',\n  'order',\n  'clarify',\n  'one',\n  'gang',\n  'members',\n  'hiding',\n  'secret',\n  'room',\n  'shooting',\n  'took',\n  'place',\n  'sally',\n  'believe',\n  'randy',\n  'killer',\n  'recognise',\n  'sheriff',\n  'slips',\n  'keys',\n  'randy',\n  'escapes',\n  'running',\n  'away',\n  'sheriff',\n  'posse',\n  'randy',\n  'conveniently',\n  'stumbles',\n  'gang',\n  'hideout',\n  'cave',\n  'responsible',\n  'murders',\n  'randy',\n  'sets',\n  'clear',\n  'name',\n  'also',\n  'bring',\n  'gang',\n  'justice',\n  'randy',\n  'rides',\n  'alone',\n  'fun',\n  'film',\n  'watch',\n  'especially',\n  'john',\n  'wayne',\n  'fan',\n  'time',\n  'far',\n  'many',\n  'flaws',\n  'impossible',\n  'ignore',\n  'film',\n  'also',\n  'extremely',\n  'dated',\n  'would',\n  'expect',\n  'terrible',\n  'camera',\n  'shooting',\n  'makes',\n  'everyone',\n  'look',\n  'like',\n  'moving',\n  'super',\n  'fast',\n  'motion',\n  'dialogue',\n  'terrible',\n  'acting',\n  'great',\n  'either',\n  'wayne',\n  'character',\n  'wooden',\n  'along',\n  'rest',\n  'cast',\n  'look',\n  'like',\n  'wooden',\n  'puppets',\n  'conducted',\n  'someone',\n  'case',\n  'director',\n  'harry',\n  'fraser',\n  'harry',\n  'fraser',\n  'helm',\n  'good',\n  'enough',\n  'job',\n  'story',\n  'paper',\n  'thin',\n  'one',\n  'help',\n  'feel',\n  'ten',\n  'minutes',\n  'missing',\n  'start',\n  'film',\n  'randy',\n  'arrives',\n  'nowhere',\n  'saloon',\n  'looking',\n  'meet',\n  'someone',\n  'explanation',\n  'randy',\n  'giving',\n  'later',\n  'turns',\n  'something',\n  'like',\n  'p',\n  'sent',\n  'investigate',\n  'claims',\n  'someone',\n  'trying',\n  'take',\n  'town',\n  'honest',\n  'really',\n  'pick',\n  'time',\n  'hoping',\n  'movie',\n  'end',\n  'said',\n  'find',\n  'film',\n  'completely',\n  'terrible',\n  'enjoyed',\n  'found',\n  'quite',\n  'fun',\n  'times',\n  'really',\n  'great',\n  'film',\n  'really',\n  'worth',\n  'watching',\n  'tracking',\n  'overall',\n  'randy',\n  'rides',\n  'alone',\n  'incredibly',\n  'dated',\n  'tiresome',\n  'western',\n  'redeeming',\n  'qualities',\n  'fun',\n  'overall',\n  'great',\n  'movie',\n  'certainly',\n  'one',\n  'wayne',\n  'weaker',\n  'outings'],\n ['hard',\n  'justice',\n  'excellent',\n  'action',\n  'movie',\n  'whole',\n  'movie',\n  'really',\n  'nothing',\n  'shooting',\n  'fighting',\n  'people',\n  'say',\n  'make',\n  'shoot',\n  'em',\n  'ups',\n  'like',\n  'use',\n  'well',\n  'one',\n  'really',\n  'hard',\n  'core',\n  'david',\n  'bradley',\n  'really',\n  'good',\n  'character',\n  'takes',\n  'pounding',\n  'movie',\n  'gets',\n  'hit',\n  'stick',\n  'dozen',\n  'times',\n  'gets',\n  'stabbed',\n  'back',\n  'coma',\n  'three',\n  'days',\n  'wakes',\n  'fights',\n  'gets',\n  'beat',\n  'recovers',\n  'ready',\n  'action',\n  'character',\n  'incredibly',\n  'tuff',\n  'charles',\n  'napier',\n  'good',\n  'well',\n  'arguably',\n  'steals',\n  'show',\n  'vernen',\n  'wells',\n  'good',\n  'professor',\n  'toru',\n  'tanaka',\n  'short',\n  'uncredited',\n  'role',\n  'hard',\n  'justice',\n  'action',\n  'truly',\n  'awesome',\n  'gun',\n  'fights',\n  'huge',\n  'stand',\n  'like',\n  'scene',\n  'beginning',\n  'cars',\n  'getting',\n  'blown',\n  'flipping',\n  'mid',\n  'air',\n  'much',\n  'happens',\n  'minute',\n  'run',\n  'time',\n  'action',\n  'fans',\n  'blown',\n  'away',\n  'fire',\n  'power',\n  'fighting',\n  'film',\n  'offer',\n  'hard',\n  'justice',\n  'movie',\n  'easy',\n  'locate',\n  'video',\n  'store',\n  'see',\n  'sale',\n  'buy',\n  'movie',\n  'big',\n  'keeper',\n  'plus',\n  'box',\n  'cool',\n  'ton',\n  'action',\n  'seen',\n  'believed',\n  'look',\n  'see',\n  'find',\n  'good',\n  'deals',\n  'ebay',\n  'half',\n  'com',\n  'amazon',\n  'com',\n  'z',\n  'shops',\n  'market',\n  'place',\n  'sellers',\n  'strongly',\n  'recommend',\n  'action',\n  'movie',\n  'fan',\n  'loves',\n  'shoot',\n  'ems',\n  'fighting',\n  'movies',\n  'disappointed',\n  'movies',\n  'look',\n  'like',\n  'true',\n  'non',\n  'stop',\n  'action',\n  'flick',\n  'fails',\n  'deliver',\n  'get',\n  'hard',\n  'justice'],\n ['annoying',\n  'group',\n  'ex',\n  'students',\n  'monte',\n  'alto',\n  'international',\n  'high',\n  'school',\n  'decide',\n  'spend',\n  'night',\n  'abandoned',\n  'institution',\n  'mystery',\n  'killer',\n  'called',\n  'watchman',\n  'played',\n  'horror',\n  'legend',\n  'paul',\n  'naschy',\n  'murders',\n  'one',\n  'one',\n  'school',\n  'killer',\n  'features',\n  'references',\n  'teen',\n  'slasher',\n  'staples',\n  'like',\n  'scream',\n  'friday',\n  'th',\n  'climatic',\n  'twist',\n  'ending',\n  'looks',\n  'like',\n  'lilted',\n  'sixth',\n  'sense',\n  'uncertainty',\n  'whether',\n  'homicidal',\n  'watchman',\n  'alive',\n  'dead',\n  'provide',\n  'mild',\n  'interest',\n  'characters',\n  'one',\n  'dimensional',\n  'endless',\n  'scenes',\n  'walking',\n  'dark',\n  'school',\n  'corridors',\n  'really',\n  'got',\n  'nerves',\n  'presence',\n  'charismatic',\n  'paul',\n  'naschy',\n  'almost',\n  'saves',\n  'clich',\n  'slasher',\n  'flick',\n  'also',\n  'decent',\n  'gore',\n  'display',\n  'including',\n  'splendidly',\n  'bloody',\n  'beheading',\n  'nice',\n  'see',\n  'manuela',\n  'velasco',\n  'rec',\n  'fame',\n  'small',\n  'role'],\n ['thankful',\n  'small',\n  'college',\n  'town',\n  'abingdon',\n  'va',\n  'near',\n  'bristol',\n  'tn',\n  'home',\n  'famous',\n  'barter',\n  'theatre',\n  'gregory',\n  'peck',\n  'acted',\n  'managed',\n  'get',\n  'art',\n  'film',\n  'festival',\n  'togather',\n  'show',\n  'film',\n  'abingdon',\n  'two',\n  'hour',\n  'hours',\n  'live',\n  'trip',\n  'worth',\n  'every',\n  'sense',\n  'word',\n  'uzak',\n  'distant',\n  'amazing',\n  'brilliant',\n  'jarring',\n  'emotional',\n  'captivating',\n  'film',\n  'turkish',\n  'american',\n  'film',\n  'testimony',\n  'life',\n  'turkey',\n  'like',\n  'larger',\n  'scale',\n  'tells',\n  'world',\n  'like',\n  'turkish',\n  'whether',\n  'one',\n  'lives',\n  'istanbul',\n  'berlin',\n  'montreal',\n  'new',\n  'york',\n  'omaha',\n  'may',\n  'two',\n  'hours',\n  'length',\n  'opposed',\n  'five',\n  'minutes',\n  'effectively',\n  'bob',\n  'marley',\n  'song',\n  'many',\n  'wonderful',\n  'scenes',\n  'film',\n  'difficult',\n  'choose',\n  'random',\n  'one',\n  'telling',\n  'scene',\n  'takes',\n  'place',\n  'beyoglu',\n  'downtown',\n  'istanbul',\n  'cinema',\n  'title',\n  'character',\n  'played',\n  'mehmet',\n  'emin',\n  'toprak',\n  'sadly',\n  'died',\n  'car',\n  'accident',\n  'shortly',\n  'film',\n  'completion',\n  'follows',\n  'attractive',\n  'young',\n  'woman',\n  'staircase',\n  'cinema',\n  'main',\n  'auditorium',\n  'goes',\n  'see',\n  'vanilla',\n  'sky',\n  'image',\n  'tom',\n  'cruise',\n  'reflected',\n  'glass',\n  'sense',\n  'turkish',\n  'men',\n  'competing',\n  'tom',\n  'cruise',\n  'women',\n  'affections',\n  'even',\n  'though',\n  'tom',\n  'cruise',\n  'nowhere',\n  'found',\n  'beyoglu',\n  'scenes',\n  'shot',\n  'across',\n  'bosphorous',\n  'shores',\n  'also',\n  'quite',\n  'revealing',\n  'symbolize',\n  'beauty',\n  'yet',\n  'desperate',\n  'empty',\n  'gulfs',\n  'painful',\n  'fact',\n  'life',\n  'turkey',\n  'film',\n  'gulf',\n  'separates',\n  'lovers',\n  'families',\n  'simple',\n  'empty',\n  'packet',\n  'samsun',\n  'turkish',\n  'brand',\n  'cigarettes',\n  'dying',\n  'mouse',\n  'jump',\n  'screen',\n  'way',\n  'seagulls',\n  'serif',\n  'goren',\n  'yilmaz',\n  'guney',\n  'film',\n  'yol',\n  'many',\n  'guney',\n  'films',\n  'including',\n  'yol',\n  'suru',\n  'herd',\n  'completed',\n  'zeki',\n  'okten',\n  'baba',\n  'father',\n  'considered',\n  'many',\n  'best',\n  'turkish',\n  'films',\n  'ever',\n  'made',\n  'without',\n  'guney',\n  'sometimes',\n  'overblown',\n  'social',\n  'political',\n  'anger',\n  'especially',\n  'last',\n  'film',\n  'prison',\n  'drama',\n  'duvar',\n  'wall',\n  'distance',\n  'captures',\n  'essence',\n  'turkish',\n  'life',\n  'quite',\n  'remarkably',\n  'crowning',\n  'achievement',\n  'director',\n  'view',\n  'already',\n  'proclaimed',\n  'turkish',\n  'equivalent',\n  'directors',\n  'like',\n  'tarkovsky',\n  'bresson',\n  'ozu',\n  'wait',\n  'see',\n  'films'],\n ['worst',\n  'film',\n  'ever',\n  'seen',\n  'bar',\n  'none',\n  'flimsy',\n  'looking',\n  'poorly',\n  'lit',\n  'sets',\n  'laughable',\n  'acting',\n  'infantile',\n  'plot',\n  'shoddy',\n  'drawn',\n  'action',\n  'sequences',\n  'film',\n  'bad',\n  'hilarious',\n  'ten',\n  'minutes',\n  'reaching',\n  'remote',\n  'power',\n  'socket',\n  'end',\n  'film',\n  'non',\n  'experience',\n  'although',\n  'obviously',\n  'made',\n  'entire',\n  'production',\n  'acting',\n  'staff',\n  'collective',\n  'tongue',\n  'rammed',\n  'cheek',\n  'please',\n  'god',\n  'found',\n  'jack',\n  'frost',\n  'dreadful',\n  'unwatchable',\n  'quarter',\n  'hour',\n  'enough',\n  'time',\n  'must',\n  'indulging',\n  'drug',\n  'abuse'],\n ['ray',\n  'interesting',\n  'parts',\n  'technically',\n  'well',\n  'made',\n  'ray',\n  'often',\n  'sluggish',\n  'forgets',\n  'important',\n  'details',\n  'ray',\n  'life',\n  'movie',\n  'shows',\n  'us',\n  'parts',\n  'prime',\n  'successful',\n  'good',\n  'wanted',\n  'see',\n  'bits',\n  'older',\n  'life',\n  'jamie',\n  'fox',\n  'mimics',\n  'ray',\n  'charles',\n  'times',\n  'absolutely',\n  'uncanny',\n  'say',\n  'jamie',\n  'reason',\n  'got',\n  'movie',\n  'st',\n  'half',\n  'lot',\n  'better',\n  'nd',\n  'interesting',\n  'oomph',\n  'nowhere',\n  'near',\n  'sluggish',\n  'nd',\n  'big',\n  'fan',\n  'ray',\n  'charles',\n  'begin',\n  'honest',\n  'really',\n  'expectations',\n  'film',\n  'ever',\n  'ray',\n  'biggest',\n  'problem',\n  'length',\n  'could',\n  'cut',\n  'easily',\n  'relevant',\n  'scenes',\n  'ones',\n  'used',\n  'found',\n  'early',\n  'part',\n  'ray',\n  'life',\n  'starting',\n  'get',\n  'successful',\n  'interesting',\n  'humble',\n  'back',\n  'somewhat',\n  'gentleman',\n  'film',\n  'may',\n  'exaggerated',\n  'actions',\n  'got',\n  'bit',\n  'full',\n  'care',\n  'performance',\n  'jamie',\n  'fox',\n  'gives',\n  'performance',\n  'ages',\n  'ray',\n  'looks',\n  'like',\n  'ray',\n  'talks',\n  'like',\n  'ray',\n  'acts',\n  'like',\n  'ray',\n  'even',\n  'sings',\n  'like',\n  'ray',\n  'much',\n  'impression',\n  'truly',\n  'believed',\n  'ray',\n  'charles',\n  'heart',\n  'film',\n  'without',\n  'presence',\n  'film',\n  'would',\n  'complete',\n  'utter',\n  'bore',\n  'bottom',\n  'line',\n  'ray',\n  'interesting',\n  'times',\n  'dreadfully',\n  'dull',\n  'others',\n  'said',\n  'done',\n  'disappointed',\n  'routine',\n  'seemed',\n  'times',\n  'bibliographical',\n  'film',\n  'doesen',\n  'mean',\n  'automatically',\n  'oscar',\n  'worthy',\n  'jamie',\n  'fox',\n  'deserved',\n  'oscar',\n  'movie',\n  'average',\n  'best',\n  'worth',\n  'watch',\n  'keep',\n  'expectations',\n  'rather',\n  'comfortable',\n  'level'],\n ['st',\n  'watched',\n  'dir',\n  'mario',\n  'pinzauti',\n  'silly',\n  'sex',\n  'filled',\n  'master',\n  'slave',\n  'many',\n  'intimate',\n  'relations',\n  'movie',\n  'movie',\n  'seemed',\n  'care',\n  'sex',\n  'story',\n  'kind',\n  'worked',\n  'story',\n  'around',\n  'sex',\n  'laughable',\n  'dubbing',\n  'original',\n  'italian',\n  'language',\n  'version',\n  'watched',\n  'ridiculous',\n  'ending',\n  'attempt',\n  'made',\n  'give',\n  'anti',\n  'slave',\n  'statement',\n  'say',\n  'one',\n  'line',\n  'waste',\n  'time',\n  'everyone',\n  'watches',\n  'trash'],\n ['blame',\n  'guy',\n  'ritchie',\n  'late',\n  'success',\n  'ritchie',\n  'clich',\n  'ridden',\n  'lock',\n  'stock',\n  'two',\n  'smoking',\n  'barrels',\n  'triggered',\n  'series',\n  'pitiful',\n  'gangster',\n  'movies',\n  'genre',\n  'never',\n  'really',\n  'recovered',\n  'sadly',\n  'rise',\n  'footsoldier',\n  'true',\n  'story',\n  'essex',\n  'hardnut',\n  'carlton',\n  'leach',\n  'likely',\n  'reverse',\n  'trend',\n  'despite',\n  'decent',\n  'lead',\n  'performance',\n  'hartnett',\n  'film',\n  'falls',\n  'victim',\n  'familiar',\n  'east',\n  'end',\n  'stereotypes',\n  'either',\n  'busy',\n  'blowing',\n  'someone',\n  'brains',\n  'shagging',\n  'scantily',\n  'clad',\n  'blonde',\n  'fearsome',\n  'football',\n  'hooligan',\n  'eighties',\n  'key',\n  'figure',\n  'criminal',\n  'underworld',\n  'nineties',\n  'footsoldier',\n  'charts',\n  'leach',\n  'rise',\n  'ranks',\n  'thuggery',\n  'leaving',\n  'terraces',\n  'nightclubs',\n  'leach',\n  'becomes',\n  'bouncer',\n  'given',\n  'carte',\n  'blanche',\n  'kick',\n  'crap',\n  'anyone',\n  'gets',\n  'notorious',\n  'gangland',\n  'leaders',\n  'pat',\n  'tate',\n  'fairbrass',\n  'tony',\n  'tucker',\n  'stone',\n  'begins',\n  'realise',\n  'gang',\n  'warfare',\n  'cracked',\n  'fun',\n  'deconstructing',\n  'writer',\n  'director',\n  'julian',\n  'gilbey',\n  'laughable',\n  'join',\n  'dots',\n  'yob',\n  'patois',\n  'every',\n  'sentence',\n  'seems',\n  'start',\n  'gonna',\n  'fackin',\n  'fackin',\n  'occasion',\n  'went',\n  'fackin',\n  'typically',\n  'concluded',\n  'mandatory',\n  'caaaant',\n  'direction',\n  'smacks',\n  'sadism',\n  'especially',\n  'obvious',\n  'glee',\n  'gilbey',\n  'gets',\n  'filming',\n  'violent',\n  'scenes',\n  'close',\n  'case',\n  'bloody',\n  'shotgun',\n  'face',\n  'denouement',\n  'triplicate',\n  'director',\n  'gilbey',\n  'use',\n  'classic',\n  'rise',\n  'fall',\n  'gangster',\n  'narrative',\n  'condemn',\n  'footsoldier',\n  'big',\n  'screen',\n  'obscurity',\n  'fact',\n  'half',\n  'way',\n  'film',\n  'annoyingly',\n  'sidelines',\n  'leach',\n  'favour',\n  'events',\n  'culminating',\n  'infamous',\n  'shooting',\n  'tate',\n  'tucker',\n  'gilbey',\n  'sickening',\n  'appetite',\n  'scatter',\n  'shot',\n  'violence',\n  'ruins',\n  'film',\n  'whether',\n  'brick',\n  'face',\n  'axe',\n  'head',\n  'vicious',\n  'attack',\n  'train',\n  'unnecessarily',\n  'prolonged',\n  'footsoldier',\n  'much',\n  'pack',\n  'punch',\n  'leave',\n  'feeling',\n  'violated',\n  'importantly',\n  'robs',\n  'two',\n  'hours',\n  'life',\n  'get',\n  'back'],\n ...]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 학습 시킨 모델을 사용해 각 단어들을 벡터로 만들어 각 리뷰에 대한 특징값 만들기\n",
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.03479357,  0.02736662, -0.05787911, ..., -0.11714542,\n         0.10625131,  0.04002199],\n       [ 0.26386493, -0.13887046, -0.07351666, ..., -0.3478534 ,\n         0.13579936,  0.2176599 ],\n       [ 0.13160817, -0.03928975, -0.1781028 , ..., -0.27594152,\n         0.22160776,  0.05922965],\n       ...,\n       [ 0.18991743,  0.00637124, -0.09258428, ..., -0.2783701 ,\n        -0.01602164,  0.22869526],\n       [-0.28402674,  0.20114522,  0.01796369, ..., -0.10727764,\n         0.03736885,  0.0606447 ],\n       [-0.0604426 , -0.23065265, -0.12924203, ..., -0.20908059,\n         0.3107148 ,  0.11297662]], dtype=float32)"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, ..., 0, 1, 1])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs.predict(test_data_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 평가 데이터에 대해 각 리뷰를 특징 벡터로 만들었으면 이제 로지스틱 모델에 적용\n",
    "- 결과를 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'sentiment': test_predicted\n",
    "})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_answer.csv', index=False, quoting=3, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}